\section[Lecture2]{\hyperlink{toc}{Lecture2}}

Bayes Fromula
\begin{itemize}
    \item Let E, F be events
    \[ E = EF \cup EF^c \]
    \[ P(E) = P(EF) + P(EF^c)\]
    \[ P(E) = P(E|F)P(F) + P(E|F^c)P(F^c)\]
    \[ P(E) = P(E|F)P(F) + P(E|F^c)(1 - P(F))\]
    \item Example: Sp,e blood test 95\% effective in detecting a disease if in fact the person has the disease. It also has a "false positive" result s.t. gives a positive for 1\% of healthy people. Suppose 0.5\% of population has disease. 
    
    \[ D = \text{person has disease}\]
    \[ E = \text{test is positive}\]

    \item We want 
    \[ P(D|E) = \frac{P(ED)}{P(E)}\]
    \[ P(D|E) = \frac{P(E|D)P(D)}{P(E|D)P(D) + P(E|D^c)(1 - P(D))}\]
    \[ = \frac{0.95 \times 0.005}{0.95 \times 0.005 + 0.01 \times 0.995} = 0.32\]
    \item So even with a positive test, only 32\% chance of having the disease
    \item Law of Total Probability:
    \item Let $\{F_i\}$ be mutually exclusive events s.t. :
    \[ \cup^n_{i=1} F_i = S\]
    For any event E
    \[ E = E \cap (\cup^n_{i=1} F_i) = \cup^n_{i=1} (E F_i)\]
    \[ P(E) = P(\cup E F_i) = \sum_{i=1}^n P(E F_i) = \sum_{i=1}^n P(E|F_i)P(F_i)\]

    \item Idependent Events:
    
    Generally P(E|F) $\neq$ P(E)

    But if knowing F does not change prob e

    if 

    \[ P(E|F) = \frac{P(EF)}{P(F)} = P(E)\]

    \[ \boxed{P(EF) = P(E)P(F)} \]


    \item E.g. roll 2 dice 
    
    \[ E_1 \equiv \text{sum} = 6 \]
    \[ F \equiv \text{1st die} = 4 \]
    \[ E_1: \{(1,5), (2,4), (3,3), (4,2), (5,1)\} \]
    \[ F: \{(4,1), (4,2), (4,3), (4,4), (4,5), (4,6)\} \]
    \[ E_1 F = \{(4,2)\} \]
    \[ P(E_1 F) = \frac{1}{36} \] 
    \[ P(E_1) = \frac{5}{36} \] 
    \[ P(F) = \frac{6}{36}  = \frac{1}{6}\]
    \[ P(E_1 ) P(F) = \frac{5}{36} \times \frac{1}{6} = \frac{5}{216} \neq P(E_1 F)\]

    \[ E_2 \equiv \text{sum} = 7 \]
    \[ E_2: \{(1,6), (2,5), (3,4), (4,3), (5,2), (6,1)\} \]
    \[ E_2 F = \{(4,3)\} \]
    \[ P(E_2 ) = \frac{6}{36}  = \frac{1}{6}\]
    \[ P(F) = \frac{1}{6}\]
    \[ P(E_2 F) = \frac{1}{36} \]

    \item $S = \{ all possible outcomes of stochastic process, X\} $
    \[ x = \text{random variable} \]
    \[ S = \text{finite or countable infinite: discrete random variable}\]
    \[ S = \text{uncountable infinite: continuous random variable}\]

    \[ P(x_0, x_0 + dx) = \text{prob that if we take a trial of x, x is between } x_0 + dx\]
    \[ = p(x) d(x) (= f(x) dx \text{ in some books})\]
    \[ p(x) = \text{probability density function (pdf)}\]
    \item Discrete 
    \[ S = S_i \] 
    \[ p_i = \text{probability of } S_i , probability mass function (pmf)\]

    \[ 0 \leq P(S_i) \leq 1\]
    \[ 1 = P(s) \] 
    \[ 0 \leq p(x) \nleq 1 \] 
    \[ \int_0^b p(x) dx \leq 1 \]
    \[ \int_{-\infty}^{\infty} p(x) dx = 1 \]

    \item e.g. I manufactor resistors make a batch of 1k$\Omega$ resistors. I can (in principle) figure out the distribution of x.
    
    \item To describe p(x) in general we can't give $\infty$ information, so we specify
    \begin{itemize}
        \item mode $\equiv$ peak value of p(x)
        \item median $\equiv$ 50\% comulative-value
        \item mean $\equiv$ average value of x weighted by p(x)
    \end{itemize}

    \item Comulative Distribution Function (cdf) here we call 
    
    \[ F(x) = \int_{-\infty}^{x} p(x') dx' = P(X \leq x) \]
    \[ F(-\infty) = 0, F(\infty) = 1\]

    \item Expectation Value (mean):
    \item The expectation value of any function f(x) over p(x) if called E(f)   
    \[ E(f) = \int_{\Omega} f(x) p(x) dx \]
    \[ E = \text{linear operator}\]
    \[ E(af +bg) = aE(f) + bE(g)\]

    \item Expectation of powers of x:
    \[ E(x^0) = E(1) = \int 1 p(x) dx = 1\]
    \[ E(x^1) = \int x p(x) dx \equiv \mu = \text{mean value of x}\]
    \[ E(x^2) = \int x^2 p(x) dx \equiv \sigma^2 = \text{variance of x}\]

    \item Imagine having all moments $E(x^n)$ for n = 0, 1, 2, ...
    \[ \int x^n p(x) dx \text{ for } n = 0, 1, 2, ...\]

    \item Characteristic Function of p(x):
    \[ \phi(t) = \int_{-\infty}^{\infty} e^{itx} p(x) dx \]
    \[ = \text{Fourier Transform of p(x)}\]
    \[ = E(e^{itx})\]
    \[ \phi(t) = E(e^{itx}) \]
    \[ = E (1 + i t x + \frac{(itx)^2}{2!} + ... ) \]
    \[ = 1 + it E(x) + \frac{(it)^2}{2!} E(x^2) + ... \]
    
    \[ = \sum_{k=0}^{\infty} \frac{(it)^k}{k!} \mu_{k'} \]

    \[ \mu_{k'} = k^n \text{moments} \]
    
    \item Given, $\phi(t)$ we can get p(x) by inverse Fourier transform:
    
    \[ \frac{d^n \phi(t)}{dt^n} \left| \right._{t=0} = i^n \mu_{n'} \]

    \item missed something here \dots
    
    \item Central moments:
    
    \[ E((x-\mu)^n) = \int (x-\mu)^n p(x) dx \equiv \mu_n \]
    \[ \mu = \text{mean value of x} = E(x) \]
    \item 1st centeral moment
    \[ E((x-\mu)^1) = E(x) - E(\mu) = \mu - \mu = 0\]
    \item 2nd central moment
    \[ E((x-\mu)^2) \equiv V(x) = \text{variance of x} = \sigma^2 \]
    \item 3rd 
    \[ E((x-\mu)^3) = \text{skewness of x} \equiv \frac{E((x-\mu)^3)}{\sigma^3} \]
    \item 4th
    \[ E((x-\mu)^4) = \text{kurtosis of x} \equiv \frac{E((x-\mu)^4)}{\sigma^4} - 3 \]

    (-3 so that kurtosis of normal distribution is 0)
\end{itemize}
