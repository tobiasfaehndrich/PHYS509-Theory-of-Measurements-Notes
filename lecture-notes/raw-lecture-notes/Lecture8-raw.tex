\section{Lecture8}

Tuesday, October 7th 2025


\begin{itemize}
    \item Gaussian (Normal) Distribution:
          \[ G(x|\mu, \sigma) = \frac{1}{\sqrt{2 \pi} \sigma} e^{-\frac{(x - \mu)^2}{2 \sigma^2}} \]
    \item Normal: $\mu = 0$, $\sigma = 1$:
          \[ N(0,1) = \frac{1}{\sqrt{2 \pi}} e^{-\frac{x^2}{2}} \]
    \item Example: Darts on board, ~ poor player. Amount miss y independent of x (circularly symmetric).
    \item Distribution of darts: $f(x,y) = h(x) k(y)$
    \item Now go to polar coords:
          \[ g(r, \theta) \approx  f(x,y) = h(x) k(y) \]
          \[ g(r,\theta) = g(r) \]
          \[ g(r,\theta) g(r) \]
          \[ \pdv{g}{\theta} = 0 = \pdv{f}{x} \pdv{x}{\theta} + \pdv{f}{y} \pdv{y}{\theta} \]
    \item $x = r \cos{\theta}$, $y = r \sin{\theta}$
    \item $\pdv{x}{\theta} = -r \sin{\theta} = -y$, $\pdv{y}{\theta} = r \cos{\theta} = x$
    \item $ 0 = h'(x) k(y) (-y) + h(x) k'(y) x$
    \item $\frac{h'(x)}{x h(x)} = \frac{k'(y)}{y k(y)} = a = constant$
    \item $h'(x) = a x h(x)$, $k'(y) = a y k(y)$
    \item $h(x) = c e^{ax^2}$, $k(y) = d e^{ay^2}$
    \item $f(x,y) = A e^{ax^2} e^{ay^2} = A e^{a(x^2 + y^2)} = A e^{ar^2} \approx A e^{-r^2}$
    \item $G(x|\mu, \sigma) = \frac{1}{\sqrt{2 \pi} \sigma} e^{-\frac{(x - \mu)^2}{2 \sigma^2}}$
    \item $E(x) =  "\mu" = \frac{1}{\sqrt{2 \pi} \sigma} \int_{-\infty}^{\infty} x e^{-\frac{(x - \mu)^2}{2 \sigma^2}} dx = \mu$
    \item $y(\frac{x-\mu}{\sigma})$
    \item $\int_{-\infty}^{\infty} e^{-ax^2} dx = \sqrt{\frac{\pi}{a}}$
    \item $E(x) = \frac{\mu}{\sqrt{2 \pi}} \sqrt{2 \pi} = \mu$
    \item The nth central moment:
          \[ \frac{1}{\sqrt{2 \pi} \sigma} \int_{-\infty}^{\infty} (x - \mu)^n e^{-\frac{(x - \mu)^2}{2 \sigma^2}} dx \]
    \item All odd moments = 0 (symmetric about mean)
    \item $I_o(a) = \int_{-\infty}^{\infty} e^{-ay^2} dy = \sqrt{\frac{\pi}{a}}$
    \item $\dv{I_o(a)}{a} = \ldots $
    \item $\div$
    \item Generally:
          \[ \dv{^n I_o(a)}{a^n} = (-1)^n \frac{(2n)!}{n!} \frac{I_o(a)}{(2a)^n} \]
    \item $y = x -\mu $
    \item $V(y) = V(x) = \frac{1}{\sqrt{2 \pi} \sigma} \int_{-\infty}^{\infty} y^2 e^{-\frac{y^2}{2 \sigma^2}} dy = -\frac{d I_o(a)}{da} |_{a = \frac{1}{2 \sigma^2}} = \sigma^2$
          %\item Kurtosis = 4th central moment ($E((\frac{x-\mu)^4)$) / $\sigma^4 - 3$ (excess kurtosis).
    \item Want $\int y^4 e^{-ay^2} dy = \frac{^2 I_o(a)}{4 a^2}$
    \item $\ldots = 3 \sqrt{2 \pi} \sigma^4$
    \item $E((\frac{x-\mu}{\sigma})^u) = \frac{1}{\sqrt{2 \pi}\sigma} = 3 \sigma^4$
    \item $E((\frac{x-\mu}{\sigma})^4) = 3$
    \item Kurtosis $\leq \geq 3 $ (exceeds $ \geq 3$)
    \item This gives 'fat tails;,
    \item Poission $\rightarrow$ Gaussian
    \item $P(r|\lambda) = \frac{1}{r!} \lambda^r e^{-\lambda}$
    \item As $\lambda$ gets large, $P(r|\lambda)$ small for $r \ll \lambda$
    \item Stirling approximation: for r large (~near 1)
          \[ r! \approx \sqrt{2 \pi r} (\frac{r}{e})^r \]
    \item $\log(P(r|\lambda)) = -\log(r!) + r \log(\lambda) - \lambda$
    \item $-\log(\sqrt{2 \pi r}) -r \log r + r + r \log \lambda - \lambda$
    \item $= r - \lambda + r (\log \lambda - \log r) -log(\sqrt{2 \pi r})$
    \item Set $x = r - \lambda$
    \item $x + r(\log \lambda - log (\lambda ( 1+ \frac{x}{\lambda}))) - \log(\sqrt{2 \pi r})$
    \item $= x+r(\log y - \log \lambda - \log(1 + \frac{x}{\lambda})) - \log(\sqrt{2 \pi r})$
    \item $=x- (\lambda + x) \log(1 + \frac{x}{\lambda}) - \log(\sqrt{2 \pi r (1+ \frac{x}{\lambda})})$
    \item $=x- (\lambda + x) \log(1 + \frac{x}{\lambda}) - \log(\sqrt{2 \pi r }) - \log(1+\frac{x}{\lambda})$
    \item $=x \ldots $
    \item Poisson is Gaussian with variance $ = - \log r! + r \log \lambda - \lambda$
    \item Central Limit Theorem:
    \item States (roughl): If you add up enough independent random variables, with finite variance, the resulting distribution will be approximately Gaussian.
    \item $x_1 \ldots x_n$, with n independent random variables, each with mean = $\mu$, variance = $\sigma^2$. Then if we define the sample mean as $ \bar{x} = \frac{1}{n} \sum_{i=1}^{n} x_i$, then as $n \rightarrow \infty$, the distribution of $\bar{x}$ approaches a Gaussian with mean = $\mu$ and variance = $\frac{\sigma^2}{n}$.
    \item Cumulative distribution for Gaussians:
    \item $F(x) = \int_{-\infty}^{x} G(y|\mu, \sigma) dy$
    \item Define erf: error function:
          \[ \text{erf}(t) = \frac{2}{\sqrt{\pi}} \int_{0}^{t} e^{-y^2} dy \]
          \[ \text{erfc}(t) = 1 - \text{erf}(t) = \frac{2}{\sqrt{\pi}} \int_{t}^{\infty} e^{-y^2} dy \]
    \item 2-sided.
    \item 1 sigma: 68.27\%
    \item 2 sigma: 95.45\%
    \item 3 sigma: 99.73\%
    \item 5 sigma : 99.99994\%
    \item Estimators: Given a sample of size n, any function designed to estimate something about the true pdf the samples were drawn from.
\end{itemize}