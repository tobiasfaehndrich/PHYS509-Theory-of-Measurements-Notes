\section{Lecture10}

Tuesday, October 14th 2025

In this lecture, I start to not take all notes, instead I write down only the key points.
\begin{itemize}
      \item Recall difference in chi-squared formula for uncorrelated and correlated measurements.
      \item Uncorrelated:
            \[ \chi^2 = \sum_{i=1}^{n} \frac{(y_i - \mu_i)^2}{\sigma_i^2} \]
      \item Correlated (general case):
            \[ \chi^2 = \sum_{i, j}^{n} (y_i - \mu_i) V^{-1}_{ij} (y_j - \mu_j) \]
            where $V^{-1}$ is the inverse of the covariance matrix $V$.
      \item If we know  backwards E is linear transformation B or $\vec{y}$ $B=$nxn matrix of eigenvectors that diagonlizes $V$.
      \item $\vec{z} = B \vec{y}$, $V_z = B V B^T$, $\mu_z = B \mu_y$
      \item From this we get:
            \[ \boxed{\chi^2(z) = \left( \vec{z} - \vec{\mu_z} \right)^T V_z^{-1} \left( \vec{z} - \vec{\mu_z} \right) }\]

      \item Suppose we have a model of $y_i = f(x_i, \vec{\theta})$, where $x_i =$ independent variable, $\vec{\theta}=$ parameters of the model.
      \item example: $y_i = m x_i + b$, and an estimate of $\sigma_i \neq \sigma_i$

      \item Want to find best estimate of $\vec{\theta}$.
      \item Form:

            \[ \chi^2(\vec{\theta}) = \sum_{i=1}^{n} \frac{(y_i - f(x_i, \hat{\theta}))^2}{\sigma_i^2} \]
      \item The best estimators for $\mu_i$ are $f(x_i|\vec{\theta})$ and $\chi^2(\hat{\theta}) = \chi^2_{\text{min}} = \sum_i \left( \frac{y_i - f(x_i|\hat{\theta})}{\sigma_i} \right)^2$
      \item We arrive at Least Squares principle (Best estimator for $\vec{\theta}$):
            \[ \hat{\theta} = \text{arg min } \chi^2(\vec{\theta}) \]
            \[ = \text{value of } \vec{\theta} \text{ that minimizes } \chi^2(\vec{\theta}) \]
            \[ = \text{solution to: } \pdv{\chi^2}{\theta_j} = 0 \]

      \item Now $\hat{\theta}$ is also a random variable! with it's own pdf. A different sample $\{x_i, y_i\}$ $\rightarrow$ different $\hat{\theta}$.
      \item Want to look at chi-square near 0. We can look at behaviour for linear fit $f = a_0 + a_1 x + a_2 x^2 + ... + a_m x^m$
      \item $f(x_i|\vec{\theta}) = (A \vec{\theta})_i$
            \[ \chi^2 = (\vec{y} - A \vec{\theta})^T V^{-1} (\vec{y} - A \vec{\theta}) \]
            \[ \chi^2 = (y_i - A_{im} \theta_m) V_{ij}^{-1} (y_j - A_{jn} \theta_n) \]
            \[ \pdv{\chi^2}{\theta_k} = -2 (y_i - A_{im} \theta_m) V_{ij}^{-1} A_{jk} \]
            \[ \pdv{\chi^2}{\theta_k}{\theta_l} =  2 A_{il} V_{ij}^{-1} A_{jk} = 2 (A^T V^{-1} A)_{kl}\]
      \item For linear fit:

            \[ V(\hat{\theta}) = (A^T V^{-1} A)^{-1} \]
      \item for not linear:
            \[ V^{-1}(\hat{\theta}) = \frac{1}{2} \pdv[2]{\chi^2}{\theta_j}{\theta_k} \Big|_{\theta = \hat{\theta}} \]
\end{itemize}