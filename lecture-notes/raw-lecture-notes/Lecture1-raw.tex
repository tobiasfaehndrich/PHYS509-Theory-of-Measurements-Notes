\section[Introduction]{\hyperlink{toc}{Introduction}}

Tuesday, September 9th 2025

This lecture talked about the course structure, grading.


Actual Data
\begin{itemize}
      \item Stochastic -- ex:
            \begin{itemize}
                  \item muon decay
                  \item inherent stochasticity
                  \item QM
            \end{itemize}
      \item But mostly devices --how do we measure?
      \item For ex a muon lifetime experiment
            \begin{itemize}
                  \item take cosmic muon, get light and discriminate
                  \item muon decays into electron and neutrinos and electron gives light
                  \item measure time between light pulses
                  \item so many things that cause noise in the data! results change even if the same mechanism happened twice.
            \end{itemize}

      \item Experimental , trial
      \item Probability (probabalistic interpretation)
            \begin{itemize}
                  \item results of long term average of repeating an expt many times
                  \item e.g. coin flip
                        \[P(H) = \lim_{N\to\infty} \frac{n_H}{N}\]
                        \[ n(H) = \text{number of heads in N trials}\]
            \end{itemize}

      \item Modern times
            \begin{itemize}
                  \item 3 axioms
                        \begin{enumerate}
                              \item Let X be a Stochastic Variable
                                    Define sample space S ($\Omega$)
                                    \[ S = \{x_1, x_2, ...\}\]
                                    e.g.
                                    \begin{enumerate}
                                          \item coin flip:
                                                \[S = \{H, T\}\]
                                          \item roll die:
                                                \[S = \{1, 2, 3, 4, 5, 6\}\]
                                          \item S = grade in this class
                                                \[ S = \{0, 1, 2, ..., 100\}\]
                                          \item decay time of radiactive atom
                                                \[ S = [0, \infty)\]
                                    \end{enumerate}
                                    S can be finite (Binomial), countable (Poisson), or infinite (Gaussian, Uniform).


                        \end{enumerate}

            \end{itemize}
\end{itemize}

% starting to take notes in flat format of itemize

\begin{itemize}
      \item Definition of an event $E=$ subset of S
      \item e.g. 1 die roll
            \[ S = \{1, 2, 3, 4, 5, 6\}\]
            \[ E = \text{rolling an even number} = \{2, 4, 6\}\]
      \item e.g. $E=$ atom decayed by time $t_0$
            \[ S = [0, t_0] \]
      \item Operations on events
      \item Union (OR) + Intersections (AND)
      \item Let A,B, be evvents in S
            \[ E = A \cup B = \{e: e \in A \text{ or } e \in B \text{ (or both)}\}\]
            clearly set in S
      \item e.g. flip coin twice
            \[ S = \{HH, HT, TH, TT\}\]
            \[ A = \text{1st flip is H} = \{HH, HT\}\]
            \[ B = \text{2nd flip is H} = \{HH, TH\}\]
            \[ A \cup B = \{HH, HT, TH\}\]
            \[ A \cap B = \{ e | in both A and B\} = \{HH\}\]
            \[ A B = A \cap B\]
            \[ A^c = \{ e | in S not in A\} = \{TH, TT\}\]
      \item Commutative
            \[ A \cup B = B \cup A, A B = B A\]
      \item Associative
            \[ A \cup (B \cup C) = (A \cup B) \cup C , (AB)C = A(BC)\]
      \item Distributive
            \[ (A\cup B)C = AC \cup BC, A(B \cup C) = AB \cup AC\]
      \item De Morgan's Laws
            \[ (A \cup B)^c = A^c B^c, (AB)^c = A^c \cup B^c\]
      \item Kotmogorov's axioms
            A fin P on S is a probability if it satisfies
            \begin{enumerate}
                  \item $P(S) = 1$
                  \item $P(\emptyset) = 0$
                  \item For any countable sequence of disjoint events $E_1, E_2, ...$ in S
                        \[ E_i E_j = \emptyset \text{ for } i \neq j\]
                        satisfies
                        \[ P(\cup_{i=1}^{\infty} E_i) = \sum_{i=1}^{\infty} P(E_i)\]
            \end{enumerate}
      \item Now some simple consequences of the axioms
            \begin{enumerate}
                  \item \[P(\emptyset) = 0\]
                        let \[E_1 = S, E_2 = \emptyset \]
                        \[ E_1 E_2 = \emptyset\]
                        \[ P(S\cup \emptyset) = P(S) + P(\emptyset) = 1 + P(\emptyset)\]
                        \[ P(S) = 1 , \, P(\emptyset) = 0\]
                  \item \[ P(E^c) = 1 - P(E)\]
                        \[ 1 = P(S) = P(E\cup E^c ) = P(E) + P(E^c)\]
                  \item If $B \subset A$, then
                        \[ P(B) \leq P(A)\]
                        \[ A = B \cup (B^c A)\]
                        \[ P(A) = P(B \cup (B^c A))\]
                        \[ P(B) = P(A) - P(B^c A) \leq P(A)\]
                  \item \[ P(A \cup B) = P(A) + P(B) - P(AB)\]
                        If we let the areas of the venn diagram  be 1 (A), 2 (A+B), 3 (B), then
                        \[ A \cup B = 1 \cup 2 \cup 3\]
                        \[ P(A\cup B) = P(1 \cup 2 \cup 3) = P(1) + P(2) + P(3)\]
                        \[ P(A) = P(1) + P(2), P(B) = P(2) + P(3)\]
                        \[ P(A) + P(B) - P(2)  = P(1) + P(2) + P(3) = P(A \cup B)\]
                        \[ \text{equivently } P(A) + P(B) - P(AB) = P(A) + P(B) - P(AB) \]
            \end{enumerate}
      \item \[ E_i = S_i \text{ for } i = 1, 2, ... n\]
            \[ E_i E_j = \emptyset \text{ for } i \neq j\]
            \[ S = \cup_{i=1}^{n} E_i\]
            \[ P(S) = 1 = P(\cup_{i=1}^{n} E_i) = \sum_{i=1}^{n} P(E_i)\]
            \[ P (E_i) = P(E_j) \text{all equally likely}\]
            \[ 1 = \sum_{i=1}^{N} P(E_i) = N P(E_i)\]
            \[ P(E_i) = \frac{1}{N} = P(E_j)\]
            \[ N = |S| = \text{number of elements in (cardinality of) S}\]
            \[ F \text{be any event (set) in S with k elements} |F| = k\]
            \[ P(F) = P(\cup_{S_i \in F} \{E_i\}) = \sum_{i=1}^{k} P(E_j) = \sum_{i=1}^{k} \frac{1}{N} = \frac{k}{N} = \frac{|F|}{|S|}\]

      \item E.g. pokerhand of 5 cards that make a straight
            \[ S = \{ (AC, 2C, 3C, 4C, 5C), (2C, 3C, 4C, 5C, 6C), ...\}\]
            \[ S = (52 \text{ choose } 5) = \frac{52!}{5! 47!} = 2,598,960\]
      \item Event = straight = 5 consecutive cards, not of same suit, any starting card.
      \item \[ 10 ( 4^5 - 4) = 10200\]
      \item Start at Ace (A,2,3,4,5), 2(2,3,4,5,6), ..., 10(10,J,Q,K,A)
      \item not all same suit = $4^5 - 4$ (all same suit)
            \[ P (\text{straight}) = \frac{10(4^5 - 4)}{52 \text{ choose } 5} = 0.00392465\]
      \item Conditional Probability:
      \item Given 2 events E, F, sample space S
            \[ P(E) = probability of  a trial from S in E \]
            \[ P(F) = probability of a trial from S in F \]
      \item Conditional Probability of E given F has occurred
            \[ P(E|F) = \text{probability of a trial from S in E, given trial is in F} \]
      \item NOT $P(EF) = \text{probability of a trial from S in both E and F} $
      \item Need to normalize to $P(F)$, so we define:
            \[ P(E|F) = \frac{P(EF)}{P(F)} \text{ if } P(F) > 0\]
            \[ P(EF) = P(E|F) P(F)\]
      \item Flip coin 2 times
            \[ S = \{HH, HT, TH, TT\}\]
            conditional prob of HH $\equiv$ A
            given
            \begin{itemize}
                  \item first flip $= H \equiv B = \{HH, HT\}$
                  \item either flip is H $\equiv C = \{HH, HT, TH\}$
                        \[ P(A|B) = \frac{P(A B)}{P(B)} = \frac{P(\{HH\})}{P(\{HH, HT\})} = \frac{\frac{1}{4}}{\frac{1}{2}} = \frac{1}{2}\]
                        \[ P(A|C) = \frac{P(A C)}{P(C)} = \frac{P(\{HH\})}{P(\{HH, HT, TH\})} = \frac{\frac{1}{4}}{\frac{3}{4}} = \frac{1}{3}\]
            \end{itemize}
\end{itemize}
