\section{Lecture9}

Thursday, October 9th 2025

\begin{itemize}
    \item \textbf{Estimator:} A function of sample $\hat{a}(\vec{x})$ that estimates a quantity I am interested in.
    \item In principle can be anything (only limited by one's imagination)
    \item But are they good or bad? That is the art.
    \item Ideally we want:
    \item Consistent ($\lim_{n \rightarrow \infty} \hat{a} = a$), Unbiased ($\mathbb{E}(\hat{a(x)}) = a$), Efficient (smallest variance of all estimators)
    \item Example: Let $x_1, x_2, ... x_n$  be n measurement points
    \item e.g. Number of elements in a histogram bin, position of hits in detector
    \item $y_i$ are measurements at that point.
    \item $V(y_i) = \sigma_i^2$
    \item Suspect we know shape of $y_i$ histogram = Model (e.g. linear background + Gaussian signal)
    \item Number of entries $= m x_i + b + A e^{-\frac{(x_i - \mu)^2}{2 \sigma^2}}$
    \item Assume function $f(x, \vec{\theta})$.
    \item Least square Principle States
    \item best estimate for estimator $\vec{\theta}$ .
    \item is $\vec{\theta}$ which minimizes:
          \[ \chi^2 = \sum_{i=1}^{n} \frac{(y_i - f(x_i, \vec{\theta}))^2}{\sigma_i^2} \]
    \item $\chi^2$ = Chi squared
    \item Minimize by setting derivatives to 0:
          \[ \pdv{\chi^2}{\theta_j} = 0 \]


    \item $\sum_{i=1}^{n} \frac{(y_i - f(x_i, \vec{\theta}))}{\sigma_i^2} \pdv{f(x_i, \vec{\theta})}{\theta_j} = 0$
    \item Straight line fit, our predictor $f(x_i, a) = m x_i + b$
    \item $\vec{\theta} = (m, b)$
    \item $\chi^2 = \sum_{i=1}^{n} \frac{(y_i - (m x_i + b))^2}{\sigma_i^2}$
    \item $\pdv{\chi^2}{m} = -2 \sum_{i=1}^{n} \frac{(y_i - (m x_i + b))}{\sigma_i^2} x_i = 0$
    \item $\pdv{\chi^2}{b} = -2 \sum_{i=1}^{n} \frac{(y_i - (m x_i + b))}{\sigma_i^2} = 0$
    \item Two equations, two unknowns
    \item $\sigma_i \frac{x_i y_i} - m\sigma \frac{x_i^2}{\sigma_i^2} - \sigma \frac{x_i}{\sigma_i^2} = 0$


    \item $\sigma_1 = \sigma \frac{1}{\sigma^{1^2}}$

    \item some stuff

          \[ \begin{bmatrix}
                  S^2 & S_{x} \\
                  S_x & S_1
              \end{bmatrix}
              \begin{bmatrix}
                  m \\
                  b
              \end{bmatrix} \]

    \item In general we have a non-diagonal covariance matrix for $y_i$
          \[ \chi^2 = \sum_{i=1}^{n} \sum_{j=1}^{n} (y_i - f(x_i, \vec{\theta})) E_{ij} (y_j - f(x_j, \vec{\theta})) \]
          where $E = V^{-1}$ is the inverse of the covariance matrix $V$.

    \item Linear case: $f(x_i, \vec{\theta}) = $ linear function of $\theta$
          \[ \boxed{\vec{f}} = A \vec{\theta} \]

    \item missing lots of notes here.

    \item $ \chi^2 = (\vec{y} - A \vec{\theta})^T V^{-1} (\vec{y} - A \vec{\theta})$
    \item $ \chi^2 = \sum_{i,j} (y-A \vec{\theta})_i V_{ij}^{-1} (y - A \vec{\theta})_j$
    \item $\nabla \cdot \chi^2 = 0 = -2 (A^T V^{-1}\vec{y} - A^T V^{-1} A \vec{\theta})$
    \item $(A^T V^{-1} A) \vec{\theta} = A^T V^{-1} \vec{y}$
    \item $\Rightarrow \vec{\theta} = (A^T V^{-1} A)^{-1} A^T V^{-1} \vec{y}$
    \item $V(\vec{\theta})$, and $\vec{y} = B \vec{x}$
    \item $V(\vec{y}) = B V(\vec{x}) B^T$
    \item $V(\vec{\theta}) = (A^T V^{-1} A)^{-1} (A^T V^{-1} ) V(\vec{y}) ( (A^T V^{-1} A)^{-1} (A^T V^{-1}))^T$
    \item $\boxed{V(\vec{\theta}) = (A^T V^{-1} A)^{-1}}$
    \item $ \boxed{\vec{\theta} = (A^T V^{-1} A)^{-1} A^T V^{-1} \vec{y}}$
    \item Goodness of fit:

          \[ \boxed{ \chi^2 = \sum_{i=1}^{n} \frac{(y_i - f(x_i, \vec{\theta}))^2}{\sigma_i^2} } \]

          is distributed as $\chi^2$ with $n$ degrees of freedom if $y_i$ are Gaussian.

    \item If model good, chi-squared per dof $\sim 1$, if bad $\gg 1$
\end{itemize}