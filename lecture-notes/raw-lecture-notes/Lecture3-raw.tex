\section[Lecture3]{\hyperlink{toc}{Lecture 3}}

Tuesday, September 16th 2025

\begin{itemize}
    \item Bayes Theorem: for events A and B, we have
    \[ \boxed{P(A B) = P(A|B) P(B) = P(B|A) P(A) = P(B A)} \]

    \item Usually it is given in this form:
    \[ P(A|B) = \frac{P(B|A) P(A)}{P(B)} \]

    \item people argued for when you are allowed to use this theorem.

    \item Example: Monty Hall Problem (Game show with host named Monty Hall)
    \begin{itemize}
        \item There are 3 doors, behind one is a car, behind the other two are goats.
        \item You select a door, if car then you win!
        \item Twist: after you select a door, Monty opens one of the other 2 doors to reveal a goat.
        \item Question: stay or switch?
        \item Solution: use Bayes theorem
        \item Sample space: S = $\{$ $C_1$ = cgg, $C_2$ = gcg, $C_3$ = ggc$\}$
        \item Event 2 = MH opens door 2
        \item Event 3 = MH opens door 3
        \item Number such that your choice is door 1
        \item Take case $E_2$, then we want to know $P(C_1|E_2)$.
        \[ P(C_1 | E_2) = \frac{P(E_2 | C_1) P(C_1)}{P(E_2)} \]
        \item $P(C_1)= \frac{1}{3}$
        \item $P(E_2 | C_1) = \frac{1}{2}$, because if the car is behind door 1, Monty can open either door 2 or 3.
        \item $P(E_2) = \frac{1}{2}$
        \item Law of total probability: \[ P(E_2) = P(E_2 | C_1) P(C_1) + P(E_2 | C_2) P(C_2) + P(E_2 | C_3) P(C_3)  = \frac{1}{2} \cdot \frac{1}{3} + 0 \cdot \frac{1}{3} + 1 \cdot \frac{1}{3} = \frac{1}{2} \]
        \item $P(C_1 | E_2) = \frac{P(E_2 | C_1) P(C_1)}{P(E_2)} = \frac{\frac{1}{2} \frac{1}{3}}{\frac{1}{2}} = \frac{1}{3}$
        \item $P(C_1 | E_2) = \frac{1}{3}$
        \item $P(C_2 |E_2) = 0$
        \item $P(C_3 |E_2) = \frac{P(E_2 | C_3) P(C_3)}{P(E_2)} = \frac{1 \cdot \frac{1}{3}}{\frac{1}{2}} = \frac{2}{3}$
    \end{itemize}

    \item Alternate version of the problem: E = MH shows you a goat from $\{$ 2, 3 $\}$.
    \begin{itemize}
        \item We want to find $P(C_1 | E)$.
        \item $P(C_1 | E) = \frac{P(E | C_1) P(C_1)}{P(E)}$
        \item $P(C_1) = \frac{1}{3}$
        \item $P(E | C_1) = 1$ because if the car is behind door 1, Monty can open either door 2 or 3.
        \item $P(E) = 1$ by law of total probability:
        \item \[ P(E) = P(E | C_1) P(C_1) + P(E | C_2) P(C_2) + P(E | C_3) P(C_3) = 1 \cdot \frac{1}{3} + 1 \cdot \frac{1}{3} + 1 \cdot \frac{1}{3} = 1 \]
        \item $P(C_1 | E) = \frac{1 \cdot \frac{1}{3}}{1} = \frac{1}{3}$
    \end{itemize}

    \item Another version: what if MH does NOT know where the car is?
    \begin{itemize}
        \item E = MH opens $\{2,3\}$ and reveals a goat.
        \item We want to find $P(C_1 | E)$.
        \item $P(C_1 | E) = \frac{P(E | C_1) P(C_1)}{P(E)}$
        \item $P(C_1) = \frac{1}{3}$ because we picked door 1.
        \item $P(E | C_1) = \frac{1}{2}$ because if the car is behind door 1, Monty can open either door 2 or 3 since he does not know where the car is.
        \item By law of total probability:
        \[ P(E) = P(E | C_1) P(C_1) + P(E | C_2) P(C_2) + P(E | C_3) P(C_3) = 1 \cdot \frac{1}{3} + \frac{1}{2} \cdot \frac{1}{3} + \frac{1}{2}\cdot \frac{1}{3} = \frac{2}{3} \]
        \item $P(C_1 | E) = \frac{\frac{1}{3}}{\frac{2}{3}} = \frac{1}{2}$
    \end{itemize}

    \item Ok now back to normal version of the problem but now with n doors.
    \begin{itemize}
        \item you pick door 1, MH opens any door with goat behind it from 2 to n (n-1 options).
        \item $P(C_1 | E) = \frac{P(E | C_1) P(C_1)}{P(E)}$
        \item $P(E) =1$ because he can always choose a door with a goat behind it (lots of options and he knows the answers).
        \item $P(C_1) = \frac{1}{n}$
        \item $P(E | C_1) = 1$ because if the car is behind door 1, Monty can open any of the other doors.
    \end{itemize}
    \item Continuous probability distribution p(x):
    \item moments: 
    \[ E(x^n) = \int_{-\infty}^{\infty} x^n p(x) dx \]


    \begin{tabular}[c]{ll}
        mean: & $\mu = E(x)$ \\
        variance: & V(x) = $\sigma^2 = E((x-\mu)^2) = E(x^2) - \mu^2$ \\
        std dev: & $\sigma = \sqrt{\sigma^2}$
    \end{tabular}

    \item Central moments:
    \[ E(x-\mu)= E(x) - \mu = 0 \]
    \[ E((x-\mu)^2) = \sigma^2 \]
    \[ E((x-\mu)^3) = \text{skewness} \]
    \[ E((x-\mu)^4) = \text{kurtosis} \]

    \item Characteristic function:
    \begin{align}
        \Phi(t) = E(e^{itx}) &= \int_{-\infty}^{\infty} e^{itx} p(x) dx \\  
                &= \sum_{k=0}^{\infty} \frac{(it)^k}{k!} \mu_k
    \end{align}

    \[ \Phi_{\mu}(t) = E(e^{it(x-\mu)}) = E(e^{itx}) e^{-it\mu} = \Phi(t) e^{-it\mu} \]

    \begin{align}
    V(x) &= E((x-\mu)^2) \\
    &= E(x^2 - 2\mu x + \mu^2) \\
    &= E(x^2) - 2\mu E(x) + \mu^2 E(1) \\
    &= E(x^2) - 2\mu^2 + \mu^2 \\
    &= E(x^2) - \mu^2 = E(x^2) - (E(x))^2
    \end{align} 

    \item The discrete case (think roll a dice, pick a card etc) uses a probability mass function.
    \item Usually denote outcomes as e.g. r:
    \item $p_r =$ probability of outcome r.
    \item $\sum_r p_r = 1$
    \item $E(r) = \sum_r p_r r = \text{mean,}\, \mu$
    \item Variance $ V(r) = \sum_r (r-\mu)^2 p_r = E(r^2) - \mu^2$
    \item Coin flip is heads: S = $\{$H, T$\}$, 
    \item Often pick 0 to 1 as mapping: H = 0, T = 1.
    \item But in theory you can pick any two numbers a and b to be the mapping from an outcome to a number just so that at the end of the day you can calculate the mean and variance.
    \[ E(r) = ap_H + bp_T \]
    \item For continuous case we had integral:
    \[ F(x) = \int_{-\infty}^{x} f(x') dx' \]
    \item For discrete case we have sum:
    \[ F(r) = \sum_{r' \leq r} p_{r'} \]
    \item $F(x)$ is the cumulative distribution function (CDF).
    \item $F(x)$ is non-decreasing, $F(-\infty) = 0$, $F(\infty) = 1$.
    \item Distribution of multiple variables:
    \item elements belong to real vector space $\in \mathbb{R}^n$.
    \item $P(AB) \dots P(A,B)$
    \item $p(x_1, x_2, \ldots, x_n) \ge 0$ is the joint probability distribution function (PDF).
    \item $\int_{\Omega} p(\vec{x}) d^n x = 1$
    \item $E(f(\vec{x})) = \int_{\Omega} f(\vec{x}) p(\vec{x}) d^n x$
    \item $\mu_i = \int x_i p(\vec{x}) d^n x$
    \item $V(x_i) = \sigma_i^2 = \int (x_i - \mu_i)^2 p(\vec{x}) d^n x$
    \item Covariance:
    \item $V_{i,j} = E((x_i-\mu_i)(x_j-\mu_j))$
    \item $V_{i,i} = \sigma_i^2 = E((x_i-\mu_i)^2)$ variance
    \item $V_{i,j} = V_{j,i}$ symmetry
\end{itemize}



