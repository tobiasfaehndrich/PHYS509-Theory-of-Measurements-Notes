\section{Lecture7}

Thursday, October 2nd 2025

\begin{itemize}
    \item Recall last time:

          \[ B(r, n, p) = \binom{n}{r} p^r (1-p)^{n-r} \]

          \[ E(r) = np = \mu\]

          \[ V(r) = np(1-p) = \sigma^2 \]

          \[ r = \sqrt{np(1-p)} \]

          \[ p = \frac{\mu}{n} \]

    \item $\epsilon = \frac{r}{n}$
    \item Number detections  (people often forget the $(1-p)$ term):

          \[ n \epsilon \pm \sqrt{n \epsilon (1-\epsilon)} \]

    \item $\sigma_{\epsilon} = \frac{1}{n} \sigma_r = \frac{1}{n} \sqrt{r (1-\frac{r}{n})} = \frac{1}{\sqrt{n}} \sqrt{\epsilon (1-\epsilon)}$
    \item Standardized skewness:

          \[ E \left( \left( \frac{x-\mu}{\sigma} \right)^3 \right) = \frac{1-2p}{\sqrt{np(1-p)}} \]

    \item Excess kurtosis: $\frac{1-6p(1-p)}{np(1-p)}$
    \item Negative Binomial: How many n to get r successes.
    \item Bernoulli Distribution: Binomial with $n=1$.

          \[ B(r, n=1, p) = P_r = \binom{1}{r} p^r (1-p)^{1-r} = p^r (1-p)^{1-r} \]

          \[ P_0 = 1(1-p) = 1-p \]
          \[ P_1 = p(1-p)^0 = p \]

          \[ \mu  = E(r) = n p = p \]

          \[ V = \sigma^2 = np(1-p) = p(1-p) \]

          \[ E(r^k) = \sum_{r=0}^{1} r^k P_r = 0^k (1-p) + 1^k p = p \]

    \item Central moments: $E((r-p)^k)$

    \item Geometric distribution: negative binomial with $r=1$
    \item Number of trials to get first success.

          \[ G(n,p) = p (1-p)^{n-1} \]

    \item Take 1 roll of die p(i) = 1/6
    \item Success = get 4
          \[E(n) = 1/p = 6 \]

    \item $P(n \le 5) = .598 $
    \item $P(n \ge 7) = .335$

          \begin{table}[ht]
              \centering
              \begin{tabular}{c c c c}
                  \toprule
                  $n$     & $p(n)$ (exact)        & $p(n)$ (decimal) & $P(N\le n)$                          \\
                  \midrule
                  1       & $\dfrac{1}{6}$        & $0.166667$       & $0.166667$                           \\
                  2       & $\dfrac{5}{36}$       & $0.138889$       & $0.305556$                           \\
                  3       & $\dfrac{25}{216}$     & $0.115741$       & $0.421296$                           \\
                  4       & $\dfrac{125}{1296}$   & $0.096451$       & $0.517747$                           \\
                  5       & $\dfrac{625}{7776}$   & $0.080375$       & $0.598122$                           \\
                  6       & $\dfrac{3125}{46656}$ & $0.066979$       & $0.665101$                           \\
                  \hline
                  $\ge 7$ & (tail)                & $-$              & $P(N\ge 7)=(5/6)^6 \approx 0.334899$ \\
                  \bottomrule
              \end{tabular}
              \caption{Geometric distribution for the number of trials to first success with $p=1/6$.  Values rounded to 6 decimal places.}
          \end{table}

    \item Sum to 4 is 5/8

    \item Samples: A set of $N$ draws/trials from a pdf p(x) $\{ P_r \}$ is called a \textbf{sample} of size N $\{x_i \}_{i=1}^N$.
    \item Orthodox statistics: your sample is one of many psosible we can answer questions about the \textbf{ensemble} of samples.
    \item Sample
          \begin{itemize}
              \item 1: $\{ x_{1i} \}_{i=1}^N$ from pdf p(x)
              \item 2: $\{ x_{2i} \}_{i=1}^N$ from pdf p(x)
              \item ...
              \item M
          \end{itemize}
    \item $p(x|\mu, \sigma)$
    \item p = prob H, coin flip
    \item L p = prob T
    \item These are limits $p = \lim_{n\rightarrow \infty} n_H/n$

    \item Flip 10 times: exactly $2^10 = 1024 possible outcomes$
          \[ S_1 = \{ HHHHHHHHHH \} \]
          \[ S_2 = \{ HTHHHHHHHH \} \]
          \[ ... \]
    \item Given true p, calc prob of any $S_i$
          \[ P_i = \lim_{N\rightarrow \infty} \frac{n_i}{N} \]
    \item From pdf probability select k t sample space at size F
    \item Science/stats infer from sample space R to get pdf
    \item Not purely detective $\infty$ number of pdfs to 1 sample
    \item Inductive: If sample pdf is more likely
    \item Sample HHTHT ... known, from it we try to infer $p=p_H$.
    \item Poisson Distribution: limit of binomial for large n, small p
    \item $B(r,n,p) = \binom{n}{r} p^r (1-p)^{n-r}$
    \item Let $n \rightarrow \infty$, $p \rightarrow 0$ such that $np = \mu$ is constant (not 0 or $\infty$).
    \item Stirling approximation:
          \[ n! \approx \sqrt{2 \pi n}  n^n e^{-n} \]
    \item Then:
          \[ B(r,n,p) = \frac{n!}{r!(n-r)!} p^r (1-p)^{n-r} \]

    \item $r$ is finite, $n \rightarrow \infty$, $n-r \rightarrow \infty$

    \item \begin{align*}
              B(r,n,p) & = \frac{1}{r!} \frac{\sqrt{2 \pi n}}{\sqrt{2 \pi (n-r)}} \frac{n^n e^{-n}}{(n-r)^{n-r} e^{-(n-r)}} \left(\frac{\mu}{n}\right)^r \left(1-\frac{\mu}{n}\right)^{n-r} \\
                       & = \frac{1}{r!} \sqrt{\frac{n}{n-r}} \left( \frac{n}{n-r} \right)^{n-r} \frac{\mu^r}{e^r} \frac{(1-\frac{\mu}{n})^{n}}{(1-\frac{\mu}{n})^r}                         \\
                       & = \frac{1}{r!} \sqrt{\frac{n}{n-r}} \left(\text{missed a line} \right)                                                                                             \\
          \end{align*}

    \item $n\rightarrow \infty$, $\sqrt{\frac{n}{n-r}} \rightarrow 1$
    \item $\left(1-\frac{\mu}{n}\right)^n \rightarrow e^{-\mu}$
    \item $\left(1-\frac{\mu}{n}\right)^r \rightarrow 1$
    \item $\lim_{n\rightarrow \infty \\ n_p = \mu} = \lim B(r,n,p) = \frac{1}{r!} \mu^r e^{-\mu} = P(r|\mu) = \text{Poisson}$.
    \item Consider radioactive decay of some atoms,
          \begin{enumerate}
              \item Any time interval [ t, t+dt] contains at most 1 decay.
              \item Probability of decay occuring in this interval is proportional to dt.
              \item Whether or not an atom decays in the interval is independent of any other non-overlapping interval.
          \end{enumerate}

    \item from 1,2) $\Rightarrow$ $P_d(dt) = \lambda dt$
    \item Prob of $n_0$ decay in interval $P_0(dt) = 1 - \lambda dt$
    \item Probability of no decay by time $t+dt$
          \begin{align*}
              P_0(t+dt)          & = P_0(t) P_0(dt)                         \\
                                 & = P_0(t) (1 - \lambda dt)                \\
              P_0(t+dt) - P_0(t) & = -\lambda P_0(t) dt                     \\
              \frac{dP_0(t)}{dt} & = -\lambda P_0(t)                        \\
              P_0(t)             & = P_0(0) e^{-\lambda t} = e^{-\lambda t}
          \end{align*}
    \item Probability of getting r decays in time t + dt:
          \begin{align*}
              P_r(t+dt)      & = P_r(t) P_0(dt) + P_{r-1}(t) P_d(dt)             \\
                             & = P_r(t) (1 - \lambda dt) + P_{r-1}(t) \lambda dt \\
              \dv{P_r(t)}{t} & = -\lambda P_r(t) + \lambda P_{r-1}(t)            \\
          \end{align*}

    \item Write down solution to PDE:

          \[ P_r(t) = \frac{1}{r!} (\lambda t)^r e^{-\lambda t} \]
    \item Poisson with $\mu = \lambda t$.
    \item Properties

          \begin{align*}
              E(r) & = \sum_{r=0}^{\infty} r P(r,\mu)                            \\
                   & = \sum_{r=0}^{\infty} r \frac{\mu^r}{r!} e^{-\mu}           \\
                   & = \mu e^{-\mu} \sum_{r=1}^{\infty} \frac{\mu^{r-1}}{(r-1)!} \\
                   & = \mu e^{-\mu} \sum_{r=1}^{\infty} \frac{\mu^{r}}{r!}       \\
                   & = \mu e^{-\mu} e^{\mu}                                      \\
                   & = \mu
          \end{align*}

    \item $V(r) = E(r^2) - \mu^2$
          \begin{align*}
              E(r(r-1)) & = E(r^2) - \mu                                                \\
                        & = \sum_{r=2}^{\infty} r(r-1) \frac{\mu^r}{r!} e^{-\mu}        \\
                        & = \mu^2 e^{-\mu} \sum_{r=2}^{\infty} \frac{\mu^{r-2}}{(r-2)!} \\
                        & = \mu^2 e^{-\mu} e^{\mu} = \mu^2
          \end{align*}

    \item $E(r^2) - \mu = \mu^2 $
    \item $V(r) = E(r^2) - \mu^2 = \mu $
    \item Binom. $\mu = np$, $V(r) = np(1-p)=\mu$ for $p \rightarrow 0$ and $n \rightarrow \infty$.
\end{itemize}