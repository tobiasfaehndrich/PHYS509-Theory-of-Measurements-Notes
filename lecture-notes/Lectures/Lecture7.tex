\section{Thursday, October 2nd 2025}

\subsection{Review of the Binomial Distribution and Its Properties}

\begin{itemize}
      \item Recall last time:
            \[ B(r, n, p) = \binom{n}{r} p^r (1-p)^{n-r} \]
            \[ E(r) = np = \mu \]
            \[ V(r) = np(1-p) = \sigma^2 \]
            \[ r = \sqrt{np(1-p)} \]
            \[ p = \frac{\mu}{n} \]

      \item $\epsilon = \frac{r}{n}$
      \item Number of detections (people often forget the $(1-p)$ term):
            \[ n \epsilon \pm \sqrt{n \epsilon (1-\epsilon)} \]

      \item $\sigma_{\epsilon} = \frac{1}{n} \sigma_r = \frac{1}{n} \sqrt{r \left(1-\frac{r}{n}\right)} = \frac{1}{\sqrt{n}} \sqrt{\epsilon (1-\epsilon)}$

      \item Standardized skewness:
            \[ E \left[ \left( \frac{x-\mu}{\sigma} \right)^3 \right] = \frac{1-2p}{\sqrt{np(1-p)}} \]

      \item Excess kurtosis:
            \[ \frac{1-6p(1-p)}{np(1-p)} \]
\end{itemize}

\subsection{Bernoulli Distribution as a Special Case of the Binomial}

\begin{itemize}
      \item Bernoulli Distribution: Binomial with $n=1$.
            \[ B(r, n=1, p) = P_r = \binom{1}{r} p^r (1-p)^{1-r} = p^r (1-p)^{1-r} \]
            \[ P_0 = 1 - p \]
            \[ P_1 = p \]
            \[ \mu  = E(r) = np = p \]
            \[ V = \sigma^2 = np(1-p) = p(1-p) \]
            \[ E(r^k) = \sum_{r=0}^{1} r^k P_r = 0^k (1-p) + 1^k p = p \]
      \item Central moments: $E[(r-p)^k]$
\end{itemize}

\subsection{Negative Binomial and Geometric Distributions}

\begin{itemize}
      \item Negative Binomial: How many $n$ to get $r$ successes.
      \item Geometric distribution: negative binomial with $r=1$ (number of trials to get first success).
            \[ G(n,p) = p (1-p)^{n-1} \]
      \item Example: Roll a die with $p(i) = 1/6$ (success = get 4)
            \[ E(n) = \frac{1}{p} = 6 \]
      \item $P(n \le 5) = 0.598$
      \item $P(n \ge 7) = 0.335$
\end{itemize}

\subsection{Samples and the Concept of an Ensemble}

\begin{itemize}
      \item Samples: A set of $N$ draws/trials from a pdf $p(x)$, $\{ P_r \}$, is called a \textbf{sample} of size $N$: $\{x_i \}_{i=1}^N$.
      \item Orthodox statistics: your sample is one of many possible, and we can answer questions about the \textbf{ensemble} of samples.
      \item Samples:
            \begin{itemize}
                  \item 1: $\{ x_{1i} \}_{i=1}^N$ from pdf $p(x)$
                  \item 2: $\{ x_{2i} \}_{i=1}^N$ from pdf $p(x)$
                  \item ...
                  \item M
            \end{itemize}
      \item $p(x|\mu, \sigma)$
      \item $p$ = probability of $H$ (heads), $q = 1 - p$ = probability of $T$ (tails)
      \item These are limits: $p = \lim_{n\rightarrow \infty} \frac{n_H}{n}$
      \item Flip 10 times: exactly $2^{10} = 1024$ possible outcomes
            \[ S_1 = \{ HHHHHHHHHH \} \]
            \[ S_2 = \{ HTHHHHHHHH \} \]
            \[ \ldots \]
      \item Given true $p$, calculate probability of any $S_i$:
            \[ P_i = \lim_{N\rightarrow \infty} \frac{n_i}{N} \]
      \item From the pdf probability, select $k$ samples from a sample space of size $F$.
      \item Science/statistics infers from sample space $\mathcal{R}$ to get $p(x)$.
      \item Not purely deductive: $\infty$ number of pdfs map to one sample.
      \item Inductive: If sample pdf is more likely.
      \item Sample HHTHT ... known, from it we try to infer $p = p_H$.
\end{itemize}

\subsection{Poisson Distribution as a Limit of the Binomial}

\begin{itemize}
      \item Poisson Distribution: limit of binomial for large $n$, small $p$.
      \item
            \[ B(r,n,p) = \binom{n}{r} p^r (1-p)^{n-r} \]
      \item Let $n \rightarrow \infty$, $p \rightarrow 0$ such that $np = \mu$ is constant.
      \item Stirling approximation:
            \[ n! \approx \sqrt{2 \pi n}  n^n e^{-n} \]
      \item Then:
            \[ B(r,n,p) = \frac{n!}{r!(n-r)!} p^r (1-p)^{n-r} \]
      \item $r$ is finite, $n \rightarrow \infty$, $n-r \rightarrow \infty$

            \begin{align*}
                  B(r,n,p) & = \frac{1}{r!} \frac{\sqrt{2 \pi n}}{\sqrt{2 \pi (n-r)}} \frac{n^n e^{-n}}{(n-r)^{n-r} e^{-(n-r)}} \left(\frac{\mu}{n}\right)^r \left(1-\frac{\mu}{n}\right)^{n-r}          \\
                           & = \frac{1}{r!} \sqrt{\frac{n}{n-r}} \left( \frac{n}{n-r} \right)^{n-r} \frac{\mu^r}{e^r} \frac{\left(1-\frac{\mu}{n}\right)^{n}}{\left(1-\frac{\mu}{n}\right)^r}            \\
                           & = \frac{1}{r!} \sqrt{\frac{n}{n-r}} \frac{(1-\frac{r}{n})^r}{(1-\frac{r}{n})^{n}} \frac{\mu^r}{e^r} \frac{\left(1-\frac{\mu}{n}\right)^{n}}{\left(1-\frac{\mu}{n}\right)^r} \\
            \end{align*}

      \item $n\rightarrow \infty$, $\sqrt{\frac{n}{n-r}} \rightarrow 1$
      \item $\left(1-\frac{\mu}{n}\right)^n \rightarrow e^{-\mu}$
      \item $\left(1-\frac{\mu}{n}\right)^r \rightarrow 1$
      \item Then:
            \[ \lim_{n\rightarrow \infty,\, np = \mu} B(r,n,p) = \frac{1}{r!} \mu^r e^{-\mu} = P(r|\mu) = \text{Poisson} \]
\end{itemize}

\subsection{Poisson Process and Radioactive Decay}

\begin{itemize}
      \item Consider radioactive decay of some atoms:
            \begin{enumerate}
                  \item Any time interval $[t, t+dt]$ contains at most one decay.
                  \item Probability of a decay occurring in this interval is proportional to $dt$.
                  \item Whether or not an atom decays in the interval is independent of any other non-overlapping interval.
            \end{enumerate}

      \item From (1) and (2):
            \[ P_d(dt) = \lambda dt \]
      \item Probability of no decay in interval:
            \[ P_0(dt) = 1 - \lambda dt \]
      \item Probability of no decay by time $t + dt$:
            \begin{align*}
                  P_0(t+dt)          & = P_0(t) P_0(dt)                         \\
                                     & = P_0(t) (1 - \lambda dt)                \\
                  P_0(t+dt) - P_0(t) & = -\lambda P_0(t) dt                     \\
                  \frac{dP_0(t)}{dt} & = -\lambda P_0(t)                        \\
                  P_0(t)             & = P_0(0) e^{-\lambda t} = e^{-\lambda t}
            \end{align*}

      \item Probability of getting $r$ decays in time $t + dt$:
            \begin{align*}
                  P_r(t+dt)          & = P_r(t) P_0(dt) + P_{r-1}(t) P_d(dt)             \\
                                     & = P_r(t) (1 - \lambda dt) + P_{r-1}(t) \lambda dt \\
                  \frac{dP_r(t)}{dt} & = -\lambda P_r(t) + \lambda P_{r-1}(t)
            \end{align*}

      \item Solution to PDE:
            \[ P_r(t) = \frac{1}{r!} (\lambda t)^r e^{-\lambda t} \]
      \item Poisson distribution with $\mu = \lambda t$
\end{itemize}

\subsection{Moments and Variance of the Poisson Distribution}

\begin{itemize}
      \item Properties:
            \begin{align*}
                  E(r) & = \sum_{r=0}^{\infty} r P(r,\mu)                            \\
                       & = \sum_{r=0}^{\infty} r \frac{\mu^r}{r!} e^{-\mu}           \\
                       & = \mu e^{-\mu} \sum_{r=1}^{\infty} \frac{\mu^{r-1}}{(r-1)!} \\
                       & = \mu e^{-\mu} e^{\mu}                                      \\
                       & = \mu
            \end{align*}

      \item $V(r) = E(r^2) - \mu^2$
            \begin{align*}
                  E[r(r-1)] & = E(r^2) - \mu                                                \\
                            & = \sum_{r=2}^{\infty} r(r-1) \frac{\mu^r}{r!} e^{-\mu}        \\
                            & = \mu^2 e^{-\mu} \sum_{r=2}^{\infty} \frac{\mu^{r-2}}{(r-2)!} \\
                            & = \mu^2 e^{-\mu} e^{\mu} = \mu^2
            \end{align*}

      \item $E(r^2) - \mu = \mu^2$
      \item $V(r) = E(r^2) - \mu^2 = \mu$
      \item Binomial: $\mu = np$, $V(r) = np(1-p) = \mu$ for $p \rightarrow 0$ and $n \rightarrow \infty$.
\end{itemize}
