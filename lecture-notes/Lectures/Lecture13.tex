\section{Thursday, October 23rd 2025}

\subsection{Definition of the Likelihood Function}

\begin{itemize}
      \item Likelihood
            \[ \mathcal{L}(\vec{x}|\theta) = \text{probability of x and not theta} \]
      \item Model $\vec{\theta}$ with $p(\vec{x}|\vec{\theta})$
            \[ \mathcal{L} = \prod_{i=1}^{N} p(x_i|\vec{\theta}) \]
            \[ \ln \mathcal{L} = \sum_{i=1}^{N} \ln p(x_i|\vec{\theta}) \]
\end{itemize}

\subsection{Maximum Likelihood Estimation (MLE)}

\begin{itemize}
      \item Maximize likelihood to get best estimate of $\vec{\theta}$. Choose $\hat{\theta}$ such that:
            \[ \hat{\theta} = \text{argmax}_{\vec{\theta}} \mathcal{L}(\vec{x}|\vec{\theta}) \]
            \[ \pdv{\ln \mathcal{L(\vec{\theta})}}{\theta} \Big|_{\hat{\theta}} = 0 \]
\end{itemize}

\subsection{Quadratic Approximation of the Log-Likelihood}

\begin{itemize}
      \item Shape of $\vec{a}$ distribution: $\ln \mathcal{L}(a)$ around $\hat{a}$ is approximately quadratic.
      \item True $a = \hat{a}$; expand about $a_0$:
      \item Taylor expansion:
            \[ f(a) = f(a_0) + (\hat{a} - a_0) f'(a_\star) \qquad \text{where } a_{\star} \text{ is between } \hat{a} \text{ and } a_0 \]
      \item So, for $f = \pdv{\ln \mathcal{L}(a)}{a}$:
            \[ 0 = \pdv{\ln \mathcal{L}(a)}{a} \Big|_{a_0} + (\hat{a} - a_0) \pdv[2]{\ln \mathcal{L}(a)}{a} \Big|_{a_{\star}} \]
\end{itemize}

\subsection{Asymptotic Limit and Expectation Relation}

\begin{itemize}
      \item Large $n$ for consistent $\hat{a} \to a_0$:
            \[ \lim_{n \to \infty} \pdv[2]{\ln \mathcal{L}(a)}{a} \Big|_{a_{\star}} = \lim_{n \to \infty} \sum_i \pdv[2]{\ln p(x_i|a)}{a} \Big|_{a_{\star}} \approx \lim_{n} n \int p(x|a) \pdv[2]{\ln p(x|a)}{a} \Big|_{a_{\star}} dx \]
      \item Sum over samples $x_i$ drawn from $p(x|a)$:
            \[ = \lim_{n \to \infty} n E\left( \pdv[2]{\ln p(x|a)}{a} \Big|_{a_{\star}} \right) \]
            \[ = E \left( \pdv[2]{\ln \mathcal{L}}{a} \Big|_{a_{\star}} \right) \]
            \[ \hat{a} - a_0 = - \frac{\pdv{\ln \mathcal{L}(a)}{a} \Big|_{a_0}}{E\left( \pdv[2]{\ln \mathcal{L}}{a} \Big|_{a_{\star}} \right)} \]
            \[ 0 = \pdv{\ln \mathcal{L}(a)}{a} \Big|_{a_0} + (\hat{a} - a_0) \pdv[2]{\ln \mathcal{L}(a)}{a} \Big|_{a_{\star}} \]
\end{itemize}

\subsection{Normalization of the Likelihood Function}

\begin{itemize}
      \item The likelihood is normalized:
            \[ \int \mathcal{L}(\vec{x}|a) d\vec{x} = 1 \]
            \[ \Rightarrow \int \pdv{\mathcal{L}(\vec{x}|a)}{a} d\vec{x} = 0 \]
      \item Relation between $\mathcal{L}$ and $\ln \mathcal{L}$:
            \[ \pdv{\mathcal{L}}{a} = \pdv{\ln \mathcal{L}}{a} \mathcal{L} \]
            \[ \pdv{\ln \mathcal{L}}{a} = \sum_{i=1}^{n} \pdv{\ln p(x|a)}{a} \]
\end{itemize}

\subsection{Gaussian Approximation via the Central Limit Theorem}

\begin{itemize}
      \item The sum of $n$ variables with zero mean:
      \item By the Central Limit Theorem, for large $n$, $\pdv{\ln \mathcal{L}}{a}$ is Gaussian with mean 0.
            \[ \boxed{E\left(\pdv[2]{\ln \mathcal{L}}{a} \right) = - E \left( \left( \pdv{\ln \mathcal{L}}{a} \right)^2 \right) } \]
\end{itemize}

\subsection{Variance of the Estimator and the Fisher Information}

\begin{itemize}
      \item Variance of $\hat{a} - a_0$:
            \[ \text{Var}(\hat{a} - a_0) = \frac{\text{Var}\left( \pdv{\ln \mathcal{L}(a)}{a} \Big|_{a_0} \right)}{\left( E\left( \pdv[2]{\ln \mathcal{L}}{a} \Big|_{a_{\star}} \right) \right)^2} \]
            \[ = \frac{E\left( \left( \pdv{\ln \mathcal{L}(a)}{a} \Big|_{a_0} \right)^2 \right)}{\left( E\left( \pdv[2]{\ln \mathcal{L}}{a} \Big|_{a_{\star}} \right) \right)^2} \]
            \[ = -\frac{E\left( \pdv[2]{\ln \mathcal{L}}{a} \Big|_{a_0} \right)}{\left( E\left( \pdv[2]{\ln \mathcal{L}}{a} \Big|_{a_{\star}} \right) \right)^2} \]
      \item When $n \to \infty$, $a_{\star} \to a_0$ and $\hat{a} \to a_0$:
            \[ \boxed{\text{Var}(\hat{a}) = -\frac{1}{E\left( \pdv[2]{\ln \mathcal{L}}{a} \Big|_{a_0} \right)} } \]
      \item Fisher Information matrix:
            \[ E\left( \pdv[2]{\ln \mathcal{L}}{a} \right) \]
      \item For large $n$, $\hat{a} \to a_0$. Estimate $E(\cdot)$ by the observed value:
            \[ \pdv[2]{\ln \mathcal{L}}{a} \Big|_{\hat{a}} \]
      \item So the estimate of variance of $\hat{a} - a_0$ is:
            \[ \boxed{\text{Var}(\hat{a}-a_0) = -\frac{1}{\pdv[2]{\ln \mathcal{L}}{a} \Big|_{\hat{a}}} } \]
\end{itemize}

\subsection{Taylor Expansion Near the Maximum Likelihood Estimate}

\begin{itemize}
      \item Taylor expansion again:
            \[ \pdv{\ln \mathcal{L}(a)}{a} = \cancelto{0}{\pdv{\ln \mathcal{L}(a)}{a} \Big|_{\hat{a}}} + (a - \hat{a}) \pdv[2]{\ln \mathcal{L}(a)}{a} \Big|_{\hat{a}} + \ldots \]
            where $\pdv[2]{\ln \mathcal{L}(a)}{a} \Big|_{\hat{a}} = -\frac{1}{V(\hat{a})}$
            \[ \frac{-(a-\hat{a})}{V(\hat{a})} + \ldots \]
      \item Note that $V(\hat{a} - a_0) = V(\hat{a})$ because $a_0$ is constant and does not change the variance (it just shifts the distribution).
      \item So,
            \[ \ln \mathcal{L}(a) = \ldots \text{ missed this part} \]
\end{itemize}

\subsection{Goodness of Fit and the Kolmogorov–Smirnov Test}

\begin{itemize}
      \item Note that the value you get from the maximum likelihood does not give information on how good the fit is—it is just relative to other values of the parameters.
      \item Kolmogorov–Smirnov test for goodness of fit (KS):
            \begin{enumerate}
                  \item Order data points $\{t_i\}$ such that $t_0 \leq t_1 \leq t_2 \leq \ldots \leq t_N$
                  \item Form an accumulator $F$ (same model CDF $C$).
            \end{enumerate}
      \item Metric:
            \[ \text{max}|F(t_i) - C(t_i)| \]
\end{itemize}
