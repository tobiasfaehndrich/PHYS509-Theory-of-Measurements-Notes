\section{Tuesday, September 20th 2025}

\subsection{Propagation of Errors for a Single Variable}

\begin{itemize}
      \item Given $f(x)$ pdf, $\mu \equiv E(x)$, $\sigma^2 \equiv V(x) = E(x^2) - \mu^2$
      \item Know $f(x) \rightarrow g(y)$, given $y(x)$.
      \item Taylor expand $y(x)$ about mean $\mu$:
            \[ y(x) = y(\mu) + y'(\mu) (x - \mu) + \frac{1}{2!} y''(\mu) (x - \mu)^2 + \ldots \]
            \[ E(y(x)) ? \equiv \mu_y \]
            \[ E(y(x)) = E(y(\mu)) + y'(\mu) E(x - \mu) + \frac{1}{2!} y''(\mu) E((x - \mu)^2) + \ldots \]
            \[ = y(\mu) + y'(\mu) \cdot 0 + \frac{1}{2!} y''(\mu) V(x)+ \ldots \]

      \item To the 1st order:
            \[ \mu_y = E(y(x)) = y(\mu) = y(E(x)) \]
\end{itemize}

\subsection{Variance Propagation for a Single Variable}

\begin{itemize}
      \item Variance of $y$:
            \begin{align}
                  V(y) & = E((y(x)-E(y(x)))^2)                                               \\
                       & = E((y(x) - \mu_y)^2)                                               \\
                       & = E((y'(\mu)(x-\mu) + \frac{1}{2!} y''(\mu)(x-\mu)^2 + \ldots)^2)   \\
                       & = E(y'(\mu)^2 (x-\mu)^2 + y'(\mu) y''(\mu)(x-\mu)^3 + O((x-\mu)^4)) \\
                       & = y'(\mu)^2 V(x) + \ldots
            \end{align}

      \item Some relations:
            \begin{align*}
                  E(x) & \equiv \mu_x                                 \\
                  V(x) & \equiv \sigma_x^2                            \\
                  y    & = y(x)                                       \\
                  E(y) & \equiv \mu_y = y(\mu_x)                      \\
                  V(y) & \equiv \sigma_y^2 = (y'(\mu_x))^2 \sigma_x^2
            \end{align*}

            \[ \sigma_y = |y'(\mu_x)| \sigma_x \]

      \item Example: $ y = \frac{1}{x}$, $\dv{y}{x} = -\frac{1}{x^2}$
            \[ \sigma^2_y = \frac{1}{\mu_x^4} \sigma_x^2 \]
\end{itemize}

\subsection{Propagation of Errors for Multiple Variables}

\begin{itemize}
      \item Let us suppose we have $n$ variables $\{ x_i \}$, with pdf $f(\vec{x})$.
      \item Let $y_j = 1, 2, \ldots, m$ be $m$ functions of $x_i$.
      \item $y_j = y_j(x_1, x_2, \ldots, x_n)$
      \item $V_{ij}(x)_{n \times n}(\vec{x}) = \text{covariance matrix of } \{x_i\}$
      \item $V_{ij}(\vec{x}) = E((x_i - \mu_i)(x_j - \mu_j))$
      \item Taylor expand each $y_j$: $\vec{\mu} = (\mu_1, \mu_2, \ldots, \mu_n)$
      \item $y_j(\vec{x}) = y_j(\vec{\mu}) + \sum_i \left. \pdv{y_j}{x_i} \right|_{\vec{\mu}} (x_i - \mu_i) + \frac{1}{2!} \sum_{i,k} \left. \pdv[2]{y_j}{x_i}{x_k} \right|_{\vec{\mu}} (x_i - \mu_i)(x_k - \mu_k) + \ldots$
      \item $E(y_j(\vec{x})) = E(y_{j}(\vec{\mu})) + \sum \pdv{y_j}{x_i} E(x_i - \mu_i) + \ldots = y_j(\vec{\mu})$
\end{itemize}

\subsection{Covariance Propagation for Functions of Multiple Variables}

\begin{itemize}
      \item Covariance between $y_k$ and $y_l$:
            \[ E((y_k - \mu_{y_k})(y_l - \mu_{y_l})) \]
            \[ = E((y_k-y_k(\mu))(y_l - y_l(\mu))) \]
            \[ = E\left( \sum_i \left. \pdv{y_k}{x_i}\right|_{\mu} (x_i - \mu_i) \sum_j \left. \pdv{y_l}{x_j}\right|_{\mu} (x_j - \mu_j) \right) \]
            \[ = \sum_{i,j} \left. \pdv{y_k}{x_i}\right|_{\mu}  \left. \pdv{y_l}{x_j} \right|_{\mu} E((x_i - \mu_i)(x_j - \mu_j)) \]

            \[ \boxed{ V_{kl}(\vec{y})_{m \times m} = \sum_{i,j} \left. \pdv{y_k}{x_i}\right|_{\vec{\mu}}  \left. \pdv{y_l}{x_j} \right|_{\vec{\mu}} V_{ij}(\vec{x})_{n \times n} }\]

      \item Example: $x, y$ random variables,
            \[ V(x,y) = \begin{bmatrix}
                        V_{xx} & V_{xy} \\[6pt]
                        V_{yx} & V_{yy}
                  \end{bmatrix} = \begin{bmatrix}
                        \sigma_x^2                  & \rho_{xy} \sigma_x \sigma_y \\[6pt]
                        \rho_{xy} \sigma_x \sigma_y & \sigma_y^2
                  \end{bmatrix} \]
      \item $z = x + y$
      \item $V(z) = \sigma_z^2 = \left(\pdv{z}{x} \right)^2 V_{xx} + 2  \pdv{z}{x} \pdv{z}{y} V_{xy} + \left(\pdv{z}{y} \right)^2 V_{yy}$
      \item $ = \sigma_x^2 + 2 \rho_{xy} \sigma_x \sigma_y + \sigma_y^2$
      \item If $x_i$ are uncorrelated,
            \[ V_{ij} = \sigma_{i,j} \sigma_{i}^2 = \begin{bmatrix}
                        \sigma_1^2 & 0          \\[6pt]
                        0          & \sigma_2^2
                  \end{bmatrix} \]
            \[ V_{kl}(\vec{y}) = \sum_i \left. \pdv{y_k}{x_i}\right|_{\mu}  \left. \pdv{y_l}{x_i} \right|_{\mu} V_{ii}(\vec{x}) \]
            \[ \text{variance } V_{kk} = \sum_i \left(\pdv{y_k}{x_i} \right)^2 \sigma_i^2 \]
\end{itemize}

\subsection{Examples of Error Propagation in Measurements}

\begin{itemize}
      \item Example: Measuring resistances. $x_i$ independent, $z = x + y$, $x=R_1$ resistor value, $y=R_2$ resistor value, $z=R_{\text{tot}}$ total resistance.
      \item $R_1 \pm \sigma_{R_1}$
      \item Convention is to use $\sqrt{V(R)}$ as uncertainty.
      \item For a good measuring device, $E(R) = R_{\text{true}}$ $\leftarrow$ unbiased.
      \item $V(R) = \text{small}$
      \item $R_1 \pm \sigma_{R_1}$, $R_2 \pm \sigma_{R_2}$, then $\sigma_{R_{\text{tot}}} = \sqrt{\sigma_{R_1}^2 + \sigma_{R_2}^2}$
      \item $R = R_{\text{tot}} = R_1 + R_2$
      \item $z = xy$, like $I, R$
      \item $\sigma_z^2 = \left(\pdv{z}{x} \right)^2 \sigma_x^2 + \left(\pdv{z}{y} \right)^2 \sigma_y^2 = y^2 \sigma_x^2 + x^2 \sigma_y^2$
            \[ \boxed{ \left(\frac{\sigma_z}{z} \right)^2 = \left(\frac{\sigma_x}{x} \right)^2 + \left(\frac{\sigma_y}{y} \right)^2 } \]
\end{itemize}

\subsection{Matrix Formulation of Linear Error Propagation}

\begin{itemize}
      \item Formula is exact if transformation of variables is linear.
      \item $\vec{y} = A \vec{x}$, $A$ is $m \times n$ matrix, $\vec{x}$ is $n \times 1$, $\vec{y}$ is $m \times 1$.
      \item $\pdv{y_k}{x_i} = \text{constant} \Rightarrow$ higher order terms in Taylor expansion are $0$
      \item $V_{kl}(\vec{y}) = \sum_{i,j} \pdv{y_k}{x_i} \pdv{y_l}{x_j} V_{ij}(\vec{x})$
      \item Matrix notation:
      \item $V_{kl}(\vec{y})  = \sum_{i,j} A_{ki} A_{lj} V_{ij}(\vec{x})$
      \item $ = \sum_{i,j} A_{ki} V_{ij}(\vec{x}) A_{lj}$
      \item $ = \sum_{i,j} A_{ki} V_{ij} (A^T)_{jl}$
      \item $ = (A V(\vec{x}) A^T)_{kl}$
            \[ \boxed{ V(\vec{y})_{m \times m} = A_{m \times n} V(\vec{x})_{n \times n} A^T_{n \times m} } \]
\end{itemize}

\subsection{Variance of the Arithmetic Mean}

\begin{itemize}
      \item Example: Arithmetic mean. Let $x_i = n$ identical independent variables with $V(x_i) = \sigma_x^2$
      \item Set $\bar{x} = \frac{1}{n} \sum_{i=1}^{n} x_i$
      \item Recall that $V(a x) = a^2 V(x)$
      \item $V(\bar{x}) = V\left(\frac{1}{n} \sum_{i=1}^{n} x_i\right) = \frac{1}{n^2} V\left(\sum_{i=1}^{n} x_i\right) = \frac{1}{n^2} \sum_{i=1}^{n} V(x_i) = \frac{1}{n^2} n \sigma_x^2 = \frac{\sigma_x^2}{n}$
      \item If variables are different $\sigma_i^2$: $n$ measurements
      \item $\bar{x} = \frac{1}{n} \sum_{i=1}^{n} x_i$
      \item $V(\bar{x}) = \frac{1}{n^2} \sum_{i=1}^{n} V(x_i) = \frac{1}{n^2} \sum_{i=1}^{n} \sigma_i^2$
      \item $\sigma_{\bar{x}} = \frac{1}{n} \sqrt{\sum_{i=1}^{n} \sigma_i^2}$
\end{itemize}

\subsection{Example: Measuring the Period of a Sine Wave}

\begin{itemize}
      \item Example: Measure period of sine wave on scope.
      \item $T = \Delta t = t_2 - t_1$
      \item $\sigma_T^2 = \left(\pdv{\Delta t}{t_1} \right)^2 \sigma_{t}^2 + \left(\pdv{\Delta t}{t_2} \right)^2 \sigma_{t}^2 = \sigma_t^2 + \sigma_t^2 = 2 \sigma_t^2$
      \item Measure $N$ cycles, $T = \frac{1}{N} \Delta t $
      \item $ \sigma_{T^2} = \frac{1}{N^2} \sigma_{\Delta t}^2 = \frac{2}{N^2} \sigma_t^2$
\end{itemize}
