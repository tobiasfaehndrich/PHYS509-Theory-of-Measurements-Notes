\section{Tuesday, September 16th 2025}

\subsection{Bayes Theorem and Its Applications}

\begin{itemize}
      \item Bayes Theorem: for events $A$ and $B$, we have
            \[ \boxed{P(AB) = P(A|B) P(B) = P(B|A) P(A) = P(BA)} \]

      \item Usually it is given in this form:
            \[ P(A|B) = \frac{P(B|A) P(A)}{P(B)} \]

      \item People argued about when you are allowed to use this theorem.
\end{itemize}

\subsection{The Monty Hall Problem: A Bayesian Analysis}

\begin{itemize}
      \item Example: Monty Hall Problem (Game show with host named Monty Hall)
            \begin{itemize}
                  \item There are 3 doors; behind one is a car, behind the other two are goats.
                  \item You select a door; if the car is behind it, you win.
                  \item Twist: after you select a door, Monty opens one of the other 2 doors to reveal a goat.
                  \item Question: stay or switch?
                  \item Solution: use Bayes theorem.
                  \item Sample space: $S = \{ C_1 = \text{cgg},\, C_2 = \text{gcg},\, C_3 = \text{ggc} \}$
                  \item Event 2 = MH opens door 2.
                  \item Event 3 = MH opens door 3.
                  \item Number such that your choice is door 1.
                  \item Take case $E_2$, then we want to know $P(C_1|E_2)$.
                        \[ P(C_1 | E_2) = \frac{P(E_2 | C_1) P(C_1)}{P(E_2)} \]
                  \item $P(C_1)= \frac{1}{3}$
                  \item $P(E_2 | C_1) = \frac{1}{2}$ because if the car is behind door 1, Monty can open either door 2 or 3.
                  \item $P(E_2) = \frac{1}{2}$
                  \item Law of total probability:
                        \[ P(E_2) = P(E_2 | C_1) P(C_1) + P(E_2 | C_2) P(C_2) + P(E_2 | C_3) P(C_3)  = \frac{1}{2} \cdot \frac{1}{3} + 0 \cdot \frac{1}{3} + 1 \cdot \frac{1}{3} = \frac{1}{2} \]
                  \item $P(C_1 | E_2) = \frac{P(E_2 | C_1) P(C_1)}{P(E_2)} = \frac{\frac{1}{2} \frac{1}{3}}{\frac{1}{2}} = \frac{1}{3}$
                  \item $P(C_1 | E_2) = \frac{1}{3}$
                  \item $P(C_2 |E_2) = 0$
                  \item $P(C_3 |E_2) = \frac{P(E_2 | C_3) P(C_3)}{P(E_2)} = \frac{1 \cdot \frac{1}{3}}{\frac{1}{2}} = \frac{2}{3}$
            \end{itemize}
\end{itemize}

\subsection{Alternate Monty Hall Formulations}

\begin{itemize}
      \item Alternate version: $E =$ MH shows you a goat from $\{ 2, 3 \}$.
            \begin{itemize}
                  \item We want to find $P(C_1 | E)$.
                  \item $P(C_1 | E) = \frac{P(E | C_1) P(C_1)}{P(E)}$
                  \item $P(C_1) = \frac{1}{3}$
                  \item $P(E | C_1) = 1$ because if the car is behind door 1, Monty can open either door 2 or 3.
                  \item $P(E) = 1$ by law of total probability:
                        \[ P(E) = P(E | C_1) P(C_1) + P(E | C_2) P(C_2) + P(E | C_3) P(C_3) = 1 \cdot \frac{1}{3} + 1 \cdot \frac{1}{3} + 1 \cdot \frac{1}{3} = 1 \]
                  \item $P(C_1 | E) = \frac{1 \cdot \frac{1}{3}}{1} = \frac{1}{3}$
            \end{itemize}

      \item Another version: What if MH does \textit{not} know where the car is?
            \begin{itemize}
                  \item $E =$ MH opens $\{2,3\}$ and reveals a goat.
                  \item We want to find $P(C_1 | E)$.
                  \item $P(C_1 | E) = \frac{P(E | C_1) P(C_1)}{P(E)}$
                  \item $P(C_1) = \frac{1}{3}$ because we picked door 1.
                  \item $P(E | C_1) = \frac{1}{2}$ because if the car is behind door 1, Monty can open either door 2 or 3 since he does not know where the car is.
                  \item By law of total probability:
                        \[ P(E) = P(E | C_1) P(C_1) + P(E | C_2) P(C_2) + P(E | C_3) P(C_3) = 1 \cdot \frac{1}{3} + \frac{1}{2} \cdot \frac{1}{3} + \frac{1}{2}\cdot \frac{1}{3} = \frac{2}{3} \]
                  \item $P(C_1 | E) = \frac{\frac{1}{3}}{\frac{2}{3}} = \frac{1}{2}$
            \end{itemize}
\end{itemize}

\subsection{Monty Hall Generalized to n Doors}

\begin{itemize}
      \item Now back to the standard version of the problem but with $n$ doors.
            \begin{itemize}
                  \item You pick door 1, MH opens any door with a goat behind it from 2 to $n$ ($n-1$ options).
                  \item $P(C_1 | E) = \frac{P(E | C_1) P(C_1)}{P(E)}$
                  \item $P(E) = 1$ because he can always choose a door with a goat behind it (many options and he knows the answers).
                  \item $P(C_1) = \frac{1}{n}$
                  \item $P(E | C_1) = 1$ because if the car is behind door 1, Monty can open any of the other doors.
            \end{itemize}
\end{itemize}

\subsection{Continuous Probability Distributions and Moments}

\begin{itemize}
      \item Continuous probability distribution $p(x)$:
      \item Moments:
            \[ E(x^n) = \int_{-\infty}^{\infty} x^n p(x) \, dx \]

            \begin{tabular}[c]{ll}
                  mean:     & $\mu = E(x)$                                      \\
                  variance: & $V(x) = \sigma^2 = E((x-\mu)^2) = E(x^2) - \mu^2$ \\
                  std dev:  & $\sigma = \sqrt{\sigma^2}$
            \end{tabular}

      \item Central moments:
            \[ E(x-\mu)= E(x) - \mu = 0 \]
            \[ E((x-\mu)^2) = \sigma^2 \]
            \[ E((x-\mu)^3) = \text{skewness} \]
            \[ E((x-\mu)^4) = \text{kurtosis} \]

      \item Characteristic function:
            \begin{align}
                  \Phi(t) = E(e^{itx}) & = \int_{-\infty}^{\infty} e^{itx} p(x) \, dx  \\
                                       & = \sum_{k=0}^{\infty} \frac{(it)^k}{k!} \mu_k
            \end{align}

            \[ \Phi_{\mu}(t) = E(e^{it(x-\mu)}) = E(e^{itx}) e^{-it\mu} = \Phi(t) e^{-it\mu} \]

            \begin{align}
                  V(x) & = E((x-\mu)^2)                       \\
                       & = E(x^2 - 2\mu x + \mu^2)            \\
                       & = E(x^2) - 2\mu E(x) + \mu^2 E(1)    \\
                       & = E(x^2) - 2\mu^2 + \mu^2            \\
                       & = E(x^2) - \mu^2 = E(x^2) - (E(x))^2
            \end{align}
\end{itemize}

\subsection{Discrete Probability Distributions}

\begin{itemize}
      \item The discrete case (e.g., rolling a die, picking a card) uses a probability mass function.
      \item Usually denote outcomes as $r$:
      \item $p_r =$ probability of outcome $r$.
      \item $\sum_r p_r = 1$
      \item $E(r) = \sum_r p_r r = \text{mean } \mu$
      \item Variance: $ V(r) = \sum_r (r-\mu)^2 p_r = E(r^2) - \mu^2$
      \item Coin flip example: $S = \{H, T\}$.
      \item Often map to $0$ or $1$: $H = 0$, $T = 1$.
      \item But in theory you can pick any two numbers $a$ and $b$ to map outcomes, just so you can calculate mean and variance.
            \[ E(r) = ap_H + bp_T \]
\end{itemize}

\subsection{Cumulative Distribution Functions}

\begin{itemize}
      \item For continuous case:
            \[ F(x) = \int_{-\infty}^{x} f(x') \, dx' \]
      \item For discrete case:
            \[ F(r) = \sum_{r' \leq r} p_{r'} \]
      \item $F(x)$ is the cumulative distribution function (CDF).
      \item $F(x)$ is non-decreasing, $F(-\infty) = 0$, $F(\infty) = 1$.
\end{itemize}

\subsection{Multivariate Distributions and Covariance}

\begin{itemize}
      \item Distribution of multiple variables:
      \item Elements belong to real vector space $\mathbb{R}^n$.
      \item $P(AB) \dots P(A,B)$
      \item $p(x_1, x_2, \ldots, x_n) \ge 0$ is the joint probability distribution function (PDF).
      \item $\int_{\Omega} p(\vec{x}) d^n x = 1$
      \item $E(f(\vec{x})) = \int_{\Omega} f(\vec{x}) p(\vec{x}) d^n x$
      \item $\mu_i = \int x_i p(\vec{x}) d^n x$
      \item $V(x_i) = \sigma_i^2 = \int (x_i - \mu_i)^2 p(\vec{x}) d^n x$
      \item Covariance:
      \item $V_{i,j} = E((x_i-\mu_i)(x_j-\mu_j))$
      \item $V_{i,i} = \sigma_i^2 = E((x_i-\mu_i)^2)$ (variance)
      \item $V_{i,j} = V_{j,i}$ (symmetry)
\end{itemize}
