\documentclass[11pt]{article}
\usepackage{amsmath, amssymb, physics, geometry}
\geometry{margin=1in}

\begin{document}

\begin{center}
    {\LARGE \textbf{PHYS 509 – Theory of Measurements}}\\[4pt]
    {\large Enhanced Formula Sheet with Intuitive Explanations}
\end{center}

\tableofcontents

\newpage

%-------------------------------------------------------
\section*{Week 1 – Probability Basics}
\addcontentsline{toc}{section}{Week 1 – Probability Basics}

\textbf{Sample space and events:}
A sample space $S$ is the set of all possible outcomes. Events are subsets of $S$.

\textbf{Probability axioms (Kolmogorov):}
\begin{itemize}
    \item $P(E) \ge 0$
    \item $P(S) = 1$
    \item If $E_i$ are disjoint, $P\left(\bigcup_i E_i\right) = \sum_i P(E_i)$
\end{itemize}
These rules define how probability behaves and allow everything else to be derived.

\textbf{Conditional probability:}
\[
    P(A|B) = \frac{P(A \cap B)}{P(B)}
\]
This tells us how likely $A$ is if we know $B$ happened — it updates probabilities with new information.

\textbf{Law of total probability:}
\[
    P(A) = \sum_i P(A|B_i) P(B_i)
\]
We can compute $P(A)$ by breaking it into contributions from a complete set of mutually exclusive events $B_i$.

\textbf{Bayes’ theorem:}
\[
    P(B|A) = \frac{P(A|B)P(B)}{P(A)}
\]
Reverses conditional probabilities — crucial for inference from data.

\textbf{Random variables and PDFs:}
A random variable $X$ assigns a number to each outcome.
PDF $p(x)$: probability density for continuous variables.
CDF $P(X \le x) = \int_{-\infty}^x p(x') dx'$

\textbf{Mean and variance:}
\[
    E[X] = \int x\,p(x)\,dx, \quad \mathrm{Var}(X) = E[(X - E[X])^2]
\]
Mean is the “balance point,” variance measures spread.

%-------------------------------------------------------
\section*{Week 2 – Common Distributions}
\addcontentsline{toc}{section}{Week 2 – Common Distributions}

\textbf{Binomial distribution:}
\[
    P(k; n, p) = \binom{n}{k} p^k (1-p)^{n-k}
\]
For $n$ independent Bernoulli trials. Mean $= np$, variance $= np(1-p)$.

\textbf{Poisson distribution:}
\[
    P(k; \lambda) = \frac{\lambda^k e^{-\lambda}}{k!}
\]
Models rare events. Mean and variance both $= \lambda$.

\textbf{Gaussian (normal) distribution:}
\[
    p(x) = \frac{1}{\sqrt{2\pi\sigma^2}} e^{-(x - \mu)^2 / (2\sigma^2)}
\]
Arises from the central limit theorem. Symmetric, fully described by mean $\mu$ and variance $\sigma^2$.

\textbf{Exponential distribution:}
\[
    p(x) = \lambda e^{-\lambda x} \quad (x \ge 0)
\]
Describes waiting times between random events.

%-------------------------------------------------------
\section*{Week 3 – Transformations and Error Propagation}
\addcontentsline{toc}{section}{Week 3 – Transformations and Error Propagation}

\textbf{Variable transformations:}
If $y = g(x)$ and $p_X(x)$ known:
\[
    p_Y(y) = p_X(x) \left|\frac{dx}{dy}\right|
\]

\textbf{Error propagation (linear approximation):}
For $y = f(x_1, \dots, x_n)$:
\[
    \sigma_y^2 = \sum_i \left( \frac{\partial f}{\partial x_i} \right)^2 \sigma_{x_i}^2 + 2 \sum_{i<j} \frac{\partial f}{\partial x_i}\frac{\partial f}{\partial x_j} \mathrm{Cov}(x_i,x_j)
\]
Small uncertainties in inputs propagate linearly to outputs.

\textbf{Covariance and correlation:}
\[
    \mathrm{Cov}(X,Y) = E[(X - E[X])(Y - E[Y])]
\]
\[
    \rho_{XY} = \frac{\mathrm{Cov}(X,Y)}{\sigma_X \sigma_Y}
\]
Covariance shows how two variables change together; correlation normalizes it to $\pm 1$.

%-------------------------------------------------------
\section*{Week 4 – Joint and Conditional Distributions}
\addcontentsline{toc}{section}{Week 4 – Joint and Conditional Distributions}

\textbf{Joint PDF:}
\[
    p(x,y)
\]
Describes probability density over two variables simultaneously.

\textbf{Marginal distributions:}
\[
    p(x) = \int p(x,y)\, dy
\]

\textbf{Conditional distributions:}
\[
    p(x|y) = \frac{p(x,y)}{p(y)}
\]

\textbf{Independence:}
$X$ and $Y$ independent $\iff p(x,y) = p(x)p(y)$.

%-------------------------------------------------------
\section*{Week 5 – Bayesian Inference}
\addcontentsline{toc}{section}{Week 5 – Bayesian Inference}

\textbf{Posterior:}
\[
    p(\theta|D) = \frac{p(D|\theta)p(\theta)}{p(D)}
\]
Updates our belief about parameter $\theta$ given data $D$.

\textbf{Evidence (marginal likelihood):}
\[
    p(D) = \int p(D|\theta)p(\theta)\, d\theta
\]
Acts as a normalization factor.

\textbf{Maximum a posteriori (MAP):}
\[
    \hat{\theta}_{MAP} = \arg\max_\theta p(\theta|D)
\]

\textbf{Maximum likelihood (ML):}
\[
    \hat{\theta}_{ML} = \arg\max_\theta p(D|\theta)
\]

MAP incorporates prior knowledge; ML relies only on data.

%-------------------------------------------------------
\section*{Week 6 – Parameter Estimation and Likelihood}
\addcontentsline{toc}{section}{Week 6 – Parameter Estimation and Likelihood}

\textbf{Likelihood function:}
\[
    L(\theta) = p(D|\theta)
\]
Not a PDF over data but a function of $\theta$ given fixed data.

\textbf{Log-likelihood:}
\[
    \ln L(\theta) = \sum_i \ln p(x_i | \theta)
\]
Often maximized instead of $L$ for numerical stability.

\textbf{Fisher information:}
\[
    I(\theta) = E\left[ \left( \frac{\partial \ln L}{\partial \theta} \right)^2 \right]
\]

\textbf{Cramér–Rao bound:}
\[
    \mathrm{Var}(\hat{\theta}) \ge \frac{1}{I(\theta)}
\]
Sets a lower limit on the variance of unbiased estimators.

%-------------------------------------------------------
\section*{Week 7 – Confidence Intervals and Hypothesis Testing}
\addcontentsline{toc}{section}{Week 7 – Confidence Intervals and Hypothesis Testing}

\textbf{Confidence interval:}
An interval that will contain the true parameter a certain fraction of the time if the experiment is repeated.

\textbf{Test statistic:}
A quantity computed from data to decide between hypotheses.

\textbf{p-value:}
Probability of obtaining a result as extreme or more extreme than observed under the null hypothesis.

\textbf{Significance level $\alpha$:}
Reject $H_0$ if $p < \alpha$.

%-------------------------------------------------------
\section*{Week 8 – Chi-Squared and Goodness of Fit}
\addcontentsline{toc}{section}{Week 8 – Chi-Squared and Goodness of Fit}

\textbf{Chi-squared statistic:}
\[
    \chi^2 = \sum_{i} \frac{(O_i - E_i)^2}{\sigma_i^2}
\]
Measures how well observed data $O_i$ matches expected $E_i$.

\textbf{Reduced chi-squared:}
\[
    \chi^2_\nu = \frac{\chi^2}{\nu}
\]
Should be $\sim 1$ for a good fit.

%-------------------------------------------------------
\section*{Week 9 – Linear Regression}
\addcontentsline{toc}{section}{Week 9 – Linear Regression}

\textbf{Best-fit slope and intercept:}
\[
    m = \frac{\sum (x_i - \bar{x})(y_i - \bar{y})}{\sum (x_i - \bar{x})^2}, \quad
    b = \bar{y} - m\bar{x}
\]
Find the straight line that minimizes squared errors.

\textbf{Uncertainties:}
\[
    \sigma_m^2 = \frac{\sigma^2}{\sum (x_i - \bar{x})^2}, \quad
    \sigma_b^2 = \sigma^2 \left[ \frac{1}{N} + \frac{\bar{x}^2}{\sum (x_i - \bar{x})^2} \right]
\]

%-------------------------------------------------------
\section*{Week 10 – Covariance Matrices and Transformations}
\addcontentsline{toc}{section}{Week 10 – Covariance Matrices and Transformations}

\textbf{Covariance matrix:}
\[
    \Sigma = \begin{pmatrix}
        \sigma_x^2        & \mathrm{Cov}(x,y) \\
        \mathrm{Cov}(y,x) & \sigma_y^2
    \end{pmatrix}
\]
Encodes uncertainties and correlations in multiple variables.

\textbf{Linear transformations:}
If $\mathbf{y} = A\mathbf{x}$:
\[
    \Sigma_y = A \Sigma_x A^T
\]

%-------------------------------------------------------
\section*{Week 11 – Multivariate Gaussian and Measurement Ellipses}
\addcontentsline{toc}{section}{Week 11 – Multivariate Gaussian and Measurement Ellipses}

\textbf{Multivariate normal:}
\[
    p(\mathbf{x}) = \frac{1}{\sqrt{(2\pi)^k |\Sigma|}} \exp\left[ -\frac{1}{2} (\mathbf{x} - \mu)^T \Sigma^{-1} (\mathbf{x} - \mu) \right]
\]

\textbf{Uncertainty ellipses:}
Contours of constant $\chi^2$ form ellipses:
\[
    (\mathbf{x} - \mu)^T \Sigma^{-1} (\mathbf{x} - \mu) = c
\]
$c$ chosen from $\chi^2$ distribution for desired confidence.

%-------------------------------------------------------
\section*{Week 12 – Advanced Topics and Review}
\addcontentsline{toc}{section}{Week 12 – Advanced Topics and Review}

\textbf{Tchebyshev’s inequality:}
\[
    P(|X - \mu| \ge k\sigma) \le \frac{1}{k^2}
\]
Holds for any distribution with finite variance — shows most probability is near the mean.

\textbf{Central Limit Theorem:}
The sum of many independent random variables tends to a normal distribution, regardless of their original distribution.

\textbf{Bayesian vs Frequentist:}
Bayesian treats parameters as random variables with priors; frequentist treats them as fixed but unknown.

\end{document}
